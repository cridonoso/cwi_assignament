{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neon/Documents/cwi_assignament\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assuigment: Generative AI for Insight Retrieval from Structured Data\n",
    "#### Cristobal Donoso Oliva \n",
    "##### Nov 27th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neon/miniconda3/envs/test/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import pickle\n",
    "import ast\n",
    "import os\n",
    "\n",
    "from src.pipelines.naive import naive_pipeline\n",
    "from src.encode import encode_corpus_query, encode_from_documents\n",
    "from src.utils import format_table, get_accuracy, fix_duplicated_columns\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the `corpus.parquet` and `queries.parquet` files, which contain the tables and desired queries to be answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1001, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_parquet('./data/corpus.parquet')\n",
    "queries = pd.read_parquet('./data/queries.parquet')\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pandas dataframes that have been saved lose their data types, it is necessary to format each row to be a proper table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Award</th>\n",
       "      <th>Nominee</th>\n",
       "      <th>Category</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>DJ Magazine Awards</td>\n",
       "      <td>Dyro</td>\n",
       "      <td>Top 100 DJs</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>DJ Magazine Awards</td>\n",
       "      <td>Dyro</td>\n",
       "      <td>Top 100 DJs</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>DJ Magazine Awards</td>\n",
       "      <td>Dyro</td>\n",
       "      <td>Top 100 DJs</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>DJ Magazine Awards</td>\n",
       "      <td>Dyro</td>\n",
       "      <td>Top 100 DJs</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year               Award Nominee     Category Result\n",
       "0  2013  DJ Magazine Awards    Dyro  Top 100 DJs     30\n",
       "1  2014  DJ Magazine Awards    Dyro  Top 100 DJs     27\n",
       "2  2015  DJ Magazine Awards    Dyro  Top 100 DJs     27\n",
       "3  2016  DJ Magazine Awards    Dyro  Top 100 DJs     93"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus['table'] = corpus['table'].apply(lambda x: format_table(x)) \n",
    "corpus.iloc[2]['table']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a Query/Answer tuple from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:Who representing what nation finished fifth and sixth in the swimming at the 2012 Summer Olympics final -- Women's 800 metre freestyle and at what are their times?\n",
      "A:In the final Women's 800 metre freestyle, Denmark's Lotte Friis finished fifth in 8:23.86 and Hungarian Boglárka Kapás sixth in 8:23.89.\n"
     ]
    }
   ],
   "source": [
    "idx = queries.sample()['database_id'].values[0]\n",
    "print('Q:{}\\nA:{}'.format(queries[queries['database_id'] == idx]['query'].values[0],\n",
    "queries[queries['database_id'] == idx]['answer'].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG PIPELINES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we are going to use our custom pipelines, but first of all we need to check for errors in table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['table'] = corpus['table'].apply(lambda x: fix_duplicated_columns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum table length:  11157\n"
     ]
    }
   ],
   "source": [
    "MAXLENTOKEN = [len(str(x.to_html())) for x in corpus['table']]\n",
    "print('Maximum table length: ', np.max(MAXLENTOKEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we split tables into records or table rows. This is useful for controlling the maximum number of tokens while preserving contextual and semantic information (most LLMs truncate the input when they exceed the number of tokens).\n",
    "\n",
    "Even though this approach seems to be better than using all the tables, we didn't obtain better performance and it also increases the computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i, row in corpus.iterrows():\n",
    "    values = row['table'].to_dict(orient='records')\n",
    "    values = [str({**{'table_id': row['database_id']}, **x, **row['context']}) for x in values]\n",
    "    documents+=values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token lenght: 1823\n",
      "Number of documents with lenght less than 384: 9223/13678 (67.43%)\n"
     ]
    }
   ],
   "source": [
    "doclen = [len(d) for d in documents]\n",
    "i = np.argmax(doclen)\n",
    "nless = np.sum(np.less(doclen, 256))\n",
    "\n",
    "print('Max token lenght: {}\\nNumber of documents with lenght less than 384: {}/{} ({:.2f}%)'.format(max(doclen), \n",
    "                                                                                          nless, \n",
    "                                                                                          len(doclen),\n",
    "                                                                                          nless/len(doclen)*100\n",
    "                                                                                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Getting embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For getting embeddings we transform tables to HTML strings. We chose HTML since LLM are often trained on web content, so it is likely that they are more familiar with this format.\n",
    "\n",
    "Context information from `corpus` database is also important. We encode `corpus['table']` and `corpus['context']` separately, then we average both representation to create the final embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_embeddings, naive_queries_embeddings = encode_corpus_query(corpus, queries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Naive solution Accuracy: 0.74'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = corpus['database_id'].astype(str).to_list()\n",
    "\n",
    "naive_top_five = naive_pipeline(naive_embeddings, naive_queries_embeddings, dbids=np.array(ids, dtype='int'))\n",
    "'Naive solution Accuracy: {}'.format(np.mean(get_accuracy(queries, naive_top_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks-based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_file = './data/embeddings.pickle'\n",
    "if os.path.exists(stored_file):\n",
    "    with open(stored_file, 'rb') as file:\n",
    "        chunks = pickle.load(file)\n",
    "else:\n",
    "    embeddings, queries_embeddings = encode_from_documents(documents, queries) \n",
    "    with open('./data/embeddings.pickle', 'wb') as file:\n",
    "        chunks = {'doc_emb': embeddings, \n",
    "                   'qry_emb':queries_embeddings, \n",
    "                   'documents':documents, \n",
    "                   'queries':queries}\n",
    "        pickle.dump(chunks, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chunks-based solution Accuracy: 0.44'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_top_five = naive_pipeline(chunks['doc_emb'], chunks['qry_emb'], dbids=np.array(ids, dtype='int'))\n",
    "'Chunks-based solution Accuracy: {}'.format(np.mean(get_accuracy(queries, chunks_top_five)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ChromaDB for indexing. Here we use the embeddings obtained by the naive version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "try:\n",
    "    collection = chroma_client.create_collection(name=\"vec_db\", metadata={\"hnsw:space\": \"cosine\"} )\n",
    "except:\n",
    "    chroma_client.delete_collection(\"vec_db\")\n",
    "    collection = chroma_client.create_collection(name=\"vec_db\", metadata={\"hnsw:space\": \"cosine\"} )\n",
    "\n",
    "collection.add(\n",
    "    embeddings=naive_embeddings,\n",
    "    ids=corpus['database_id'].astype(str).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_embeddings=queries_embeddings, n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chroma solution Accuracy: 0.62'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accval = np.mean(get_accuracy(queries, np.array(results['ids'], dtype='int')))\n",
    "'Chroma solution Accuracy: {:.2f}'.format(accval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we use exactly the same embeddings the accuracy decreased. It can be the effect of the searching algorithm used by Chroma or the distance metric it is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
